{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Business and Data Analysis Chatbot\n",
        "### LangChain, HuggingFace, SQL, RAG\n",
        "\n",
        "LLMs are great for many general tasks.\n",
        "\n",
        "But they're even more powerful when integrated with rich SQL data and time-series prediction capabilities.\n",
        "\n",
        "I wanted to create a conversational assistant that can help analyze a database of real-estate/home prices, and generate forecasts based on historical data as well.\n",
        "\n",
        "It should be able to answer questions like:\n",
        "\n",
        "* What was the average home rental in zip code 01012 for the past year?\n",
        "* How much has the seasonally adjusted home rent increased in Denver, CO from 2020 to 2023?\n",
        "* Based on home rents in Hampshire County, MA for the last 4 years, create a forecast for the next 12 months\n",
        "\n",
        "I'll be using:\n",
        "\n",
        "1. OpenAI's GPT-3.5 as the LLM\n",
        "2. LangChain for the chatbot\n",
        "3. Retrieval Augmented Generation (RAG) from an SQL database\n",
        "4. HuggingFace dataset for Zillow home rents\n",
        "5. Amazon's Chronos T5 model for forecasts/predictions\n",
        "\n",
        "Let's install the required packages:"
      ],
      "metadata": {
        "id": "XeDqTIjVX4h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets langchain langchain-community langchain-openai\n",
        "!pip install git+https://github.com/amazon-science/chronos-forecasting.git"
      ],
      "metadata": {
        "id": "DzAjX-rfWp-H",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then set up the Chronos model:\n",
        "\n",
        "(Note: you'll need a good GPU to run this model)"
      ],
      "metadata": {
        "id": "ft8u0N2qcAm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from chronos import ChronosPipeline\n",
        "\n",
        "pipeline = ChronosPipeline.from_pretrained(\n",
        "    \"amazon/chronos-t5-base\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "CHPweXMHWxTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick function that takes historical data and returns future predictions, alongside outputting a graph:"
      ],
      "metadata": {
        "id": "-LWrL8MocHcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def chronos_prediction(historical, length):\n",
        "    context = torch.tensor(historical)\n",
        "    forecast = pipeline.predict(context, length)\n",
        "\n",
        "    forecast_index = range(len(historical), len(historical) + length)\n",
        "    low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(historical, color=\"blue\", label=\"Historical\")\n",
        "    plt.plot(forecast_index, median, color=\"green\", label=\"Median Forecast\")\n",
        "    plt.fill_between(\n",
        "        forecast_index,\n",
        "        low,\n",
        "        high,\n",
        "        color=\"green\",\n",
        "        alpha=0.3,\n",
        "        label=\"80% prediction interval\",\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return median\n",
        "\n",
        "\n",
        "chronos_prediction(\n",
        "    [\n",
        "        347,\n",
        "        305,\n",
        "        336,\n",
        "        340,\n",
        "        318,\n",
        "        362,\n",
        "        348,\n",
        "        363,\n",
        "        435,\n",
        "        491,\n",
        "        505,\n",
        "        404,\n",
        "        359,\n",
        "        310,\n",
        "        337,\n",
        "        360,\n",
        "        342,\n",
        "        406,\n",
        "        396,\n",
        "        420,\n",
        "        472,\n",
        "        548,\n",
        "        559,\n",
        "        463,\n",
        "        407,\n",
        "        362,\n",
        "        405,\n",
        "        417,\n",
        "        391,\n",
        "        419,\n",
        "    ],\n",
        "    12,\n",
        ")"
      ],
      "metadata": {
        "id": "L5Sg5aSdXNwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Zillow database from HuggingFace:"
      ],
      "metadata": {
        "id": "veeP5EtScbH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"misikoff/zillow-viewer\", \"rentals\")[\"train\"]"
      ],
      "metadata": {
        "id": "D6AZYdIjZdmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "id": "YXwh78BBLzKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Zillow HF dataset has different rows for various zip codes, cities, counties, and MSAs.\n",
        "\n",
        "I'm only interested in the zip code data. So for a given zip code, I want to get the city, county, and state the zip code is in based on this CSV from GitHub:"
      ],
      "metadata": {
        "id": "LGzHqK0OckdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "zipData = pandas.read_csv(\n",
        "    \"https://raw.githubusercontent.com/scpike/us-state-county-zip/master/geo-data.csv\"\n",
        ")\n",
        "zipHash = {}\n",
        "\n",
        "for row in zipData.iterrows():\n",
        "    zipHash[row[1][\"zipcode\"]] = {\n",
        "        \"city\": row[1][\"city\"],\n",
        "        \"county\": row[1][\"county\"],\n",
        "        \"state\": row[1][\"state_abbr\"],\n",
        "    }\n",
        "\n",
        "zipHash[\"01012\"]"
      ],
      "metadata": {
        "id": "KCCFclB6OonM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipHash[\"01013\"]"
      ],
      "metadata": {
        "id": "TzFYY6IXOQdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to an SQLite database and create the table:"
      ],
      "metadata": {
        "id": "K0nBr22qdNak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import SQLDatabase\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///zillow.db\")\n",
        "\n",
        "db.run(\n",
        "    \"\"\"CREATE TABLE rentals (\n",
        "id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "zip VARCHAR,\n",
        "city VARCHAR,\n",
        "county VARCHAR,\n",
        "state VARCHAR,\n",
        "home_type INTEGER,\n",
        "date DATETIME,\n",
        "rent FLOAT,\n",
        "rent_adjusted FLOAT\n",
        ")\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "em1HcG2VN0lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has many missing/null columns.\n",
        "\n",
        "We need to filter the dataset where `Region Type` is `0` (those are the zip code rows) and `Rent (Smoothed)` is not empty.\n",
        "\n",
        "Some zip codes also don't have the leading 0, we need to add that.\n",
        "\n",
        "The rentals dataset has 1M+ rows, but I'm limiting them to 10K here.\n",
        "\n",
        "(This can take a while. Using `INSERT INTO` with multiple sets of values would probably be more efficient...)"
      ],
      "metadata": {
        "id": "P5aVFMiadUrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_count = 0\n",
        "for i in range(len(dataset)):\n",
        "    row = dataset[i]\n",
        "\n",
        "    if row[\"Region Type\"] != 0 or row[\"Rent (Smoothed)\"] is None:\n",
        "        continue\n",
        "\n",
        "    zip = f\"0{row['Region']}\" if len(row[\"Region\"]) < 5 else row[\"Region\"]\n",
        "    geo = zipHash.get(zip)\n",
        "\n",
        "    if geo is None:\n",
        "        continue\n",
        "\n",
        "    db.run(\n",
        "        \"INSERT INTO rentals (zip, city, county, state, home_type, date, rent, rent_adjusted) VALUES (:zip, :city, :county, :state, :home_type, :date, :rent, :rent_adjusted)\",\n",
        "        parameters={\n",
        "            \"zip\": zip,\n",
        "            \"city\": geo[\"city\"],\n",
        "            \"county\": geo[\"county\"],\n",
        "            \"state\": geo[\"state\"],\n",
        "            \"home_type\": row[\"Home Type\"],\n",
        "            \"date\": row[\"Date\"].isoformat(),\n",
        "            \"rent\": row[\"Rent (Smoothed)\"],\n",
        "            \"rent_adjusted\": row[\"Rent (Smoothed) (Seasonally Adjusted)\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    row_count += 1\n",
        "    if row_count >= 10000:\n",
        "        break"
      ],
      "metadata": {
        "id": "-KDwPB2pUKyS",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reconnect to the database so we have the updated table schema and details:"
      ],
      "metadata": {
        "id": "rDBmQCscehB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///zillow.db\")\n",
        "\n",
        "pprint(db.run(\"select * from rentals limit 5\"))\n",
        "# pprint(db.run('select count() as count, city, state from rentals group by city order by count desc limit 10'))"
      ],
      "metadata": {
        "id": "pbKQxU2BXKR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI's API key:"
      ],
      "metadata": {
        "id": "XMFpvwEZeq1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "api_key = getpass.getpass()"
      ],
      "metadata": {
        "id": "RgHO9ekIjcyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a simple in-memory hash to store the chat history/sessions (easier for demonstration).\n",
        "\n",
        "Also a function to send only the latest 10 messages to the LLM so we don't overflow its context:"
      ],
      "metadata": {
        "id": "dGIU8aG1eslp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage,\n",
        "    ToolMessage,\n",
        "    ToolMessageChunk,\n",
        ")\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(id):\n",
        "    if id not in store:\n",
        "        store[id] = ChatMessageHistory()\n",
        "    return store[id]\n",
        "\n",
        "\n",
        "def limit(input):\n",
        "    return {**input, \"messages\": input[\"messages\"][-10:]}"
      ],
      "metadata": {
        "id": "-pwKX8HWwJO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a few tools/functions for the LLM so it can:\n",
        "\n",
        "1. See the table names and schemas in the database\n",
        "2. Run SQL queries\n",
        "3. Get current date and time (if needed to answer the user's question)\n",
        "4. Generate time-series predictions using the Chronos model\n",
        "\n",
        "Also creating a system prompt and template.\n",
        "\n",
        "(The tool descriptions provided by `SQLDatabaseToolkit` could probably be better, but I'm leaving them as in for now)"
      ],
      "metadata": {
        "id": "IrqjStcwfE8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from typing import List\n",
        "from langchain_openai import ChatOpenAI\n",
        "import json\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", openai_api_key=api_key)\n",
        "\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=model)\n",
        "sql_tools = toolkit.get_tools()\n",
        "\n",
        "sql_tools[0].name = \"query_sql_database_tool\"\n",
        "sql_tools[1].name = \"info_sql_database_tool\"\n",
        "sql_tools[2].name = \"list_sql_database_tool\"\n",
        "sql_tools[3].name = \"query_sql_checker_tool\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_current_datetime(current: bool):\n",
        "    \"\"\"Get current date and time in ISO format\"\"\"\n",
        "    return datetime.now().isoformat()\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_time_series_prediction(\n",
        "    historical_values: List[float], number_of_values_to_predict: int\n",
        "):\n",
        "    \"\"\"Use this tool to generate possible future predictions based on past time series data.\n",
        "    Provide a list of numbers as 'historical_values', and specify how many future values to predict in 'number_of_values_to_predict'\n",
        "    This tool returns the predicted list of numbers representing median trends/forecasts. It'll also output a visual graph.\n",
        "    \"\"\"\n",
        "    pred = chronos_prediction(historical_values, number_of_values_to_predict)\n",
        "    return json.dumps(pred.tolist())\n",
        "\n",
        "\n",
        "toolsHash = {\n",
        "    \"query_sql_database_tool\": sql_tools[0],\n",
        "    \"info_sql_database_tool\": sql_tools[1],\n",
        "    \"list_sql_database_tool\": sql_tools[2],\n",
        "    \"query_sql_checker_tool\": sql_tools[3],\n",
        "    \"get_current_datetime\": get_current_datetime,\n",
        "    \"get_time_series_prediction\": get_time_series_prediction,\n",
        "}\n",
        "\n",
        "model = model.bind_tools([*sql_tools, get_current_datetime, get_time_series_prediction])\n",
        "\n",
        "system_prompt = \"\"\"You are an assistant designed to help with business and data analysis.\n",
        "If the user asks for data you don't have, use the provided tools/functions to interact with a database; follow these steps:\n",
        "\n",
        "1. First, you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step\n",
        "\n",
        "2. Then query the schema of the most relevant tables\n",
        "\n",
        "3. Create a syntactically correct SQLite query\n",
        "\n",
        "4. You MUST use the tool to check/validate your query syntax before executing it. If you get an error while executing a query, rewrite the query and try again\n",
        "\n",
        "5. Run the query, look at the results, and only use this returned information to construct your final answer\n",
        "\n",
        "Guidelines:\n",
        "\n",
        "Do not use the 'multi_tool_use.parallel' tool, call each tool individually.\n",
        "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
        "You can order the results by a relevant column to return the most interesting examples in the database.\n",
        "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
        "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [SystemMessage(system_prompt), MessagesPlaceholder(variable_name=\"messages\")]\n",
        ")"
      ],
      "metadata": {
        "id": "zHbUTg2BlXUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting together the main chain and chat history.\n",
        "\n",
        "We also need to be able to execute the tools called by the LLM.\n",
        "\n",
        "You could use LangGraph's built-in agent for this, but I wanted to implement my own.\n",
        "\n",
        "I've also configured streaming support:"
      ],
      "metadata": {
        "id": "iAXVN7yOf_yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import chain, RunnableLambda\n",
        "\n",
        "chain = limit | template | model\n",
        "with_history = RunnableWithMessageHistory(\n",
        "    chain, get_session_history, input_messages_key=\"messages\"\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"cat\"}}\n",
        "\n",
        "\n",
        "def agent(input):\n",
        "    messages = input[\"messages\"]\n",
        "\n",
        "    while True:\n",
        "        response = None\n",
        "\n",
        "        for chunk in with_history.stream(\n",
        "            {**input, \"messages\": messages}, config=config\n",
        "        ):\n",
        "            yield chunk\n",
        "            if response is None:\n",
        "                response = chunk\n",
        "            else:\n",
        "                response += chunk\n",
        "\n",
        "        if response.tool_calls:\n",
        "            messages = []\n",
        "            for call in response.tool_calls:\n",
        "                tool = toolsHash.get(call[\"name\"])\n",
        "                if tool:\n",
        "                    output = tool.invoke(call[\"args\"])\n",
        "                    tool_message = ToolMessageChunk(output, tool_call_id=call[\"id\"])\n",
        "                    yield tool_message\n",
        "                    messages.append(tool_message)\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "gxDDKTHHn7FN",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, time to run the whole thing!"
      ],
      "metadata": {
        "id": "txXeCGS_gnu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(message):\n",
        "    res = None\n",
        "    stream = RunnableLambda(agent).stream(\n",
        "        {\"messages\": [HumanMessage(message)], \"language\": \"English\"}, config=config\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        if res is None:\n",
        "            res = chunk\n",
        "        else:\n",
        "            res += chunk\n",
        "\n",
        "        if chunk.content:\n",
        "            print(chunk.content, end=\"\")\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "# run('What has been the average home rent in Worcester County for the past year?')\n",
        "# run('Calculate the average home rent for zip code 07302 for years 2020 and 2023. And by what percentage has the rent changed during this time?')\n",
        "run(\n",
        "    \"Based on home rents in Newark, NJ for the last 4 years, create a prediction graph for the next 12 months\"\n",
        ")"
      ],
      "metadata": {
        "id": "H6k3M_887h3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Assuming agent and config are defined elsewhere in your code\n",
        "# from your_code import agent, config\n",
        "\n",
        "def run(message):\n",
        "    res = None\n",
        "    stream = RunnableLambda(agent).stream(\n",
        "        {\"messages\": [HumanMessage(message)], \"language\": \"English\"}, config=config\n",
        "    )\n",
        "\n",
        "    result = []\n",
        "    for chunk in stream:\n",
        "        if res is None:\n",
        "            res = chunk\n",
        "        else:\n",
        "            res += chunk\n",
        "\n",
        "        if chunk.content:\n",
        "            result.append(chunk.content)\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# Streamlit app layout\n",
        "st.title(\"Business Analysis and Prediction Agent\")\n",
        "st.write(\"Submit your query to get analysis and predictions.\")\n",
        "\n",
        "# Input field for user message\n",
        "user_input = st.text_input(\"Enter your query:\", \"\")\n",
        "\n",
        "# Add a button to submit the query\n",
        "if st.button(\"Submit Query\"):\n",
        "    if user_input:\n",
        "        # Run the agent with the user's input\n",
        "        response = run(user_input)\n",
        "\n",
        "        st.write(\"### Agent's Response:\")\n",
        "        st.write(response)\n",
        "\n",
        "        # For demonstration, if the response includes a command to plot a graph\n",
        "        if \"create a prediction graph\" in user_input.lower():\n",
        "            # Example data for plotting\n",
        "            historical_data = [1500, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 1950, 2000]\n",
        "            forecast = [2100, 2150, 2200, 2250, 2300, 2350, 2400, 2450, 2500, 2550, 2600, 2650]\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(historical_data, label='Historical Data', marker='o')\n",
        "            forecast_index = range(len(historical_data), len(historical_data) + len(forecast))\n",
        "            plt.plot(forecast_index, forecast, label='Forecast', marker='o', color='green')\n",
        "            plt.fill_between(forecast_index, [f * 0.9 for f in forecast], [f * 1.1 for f in forecast], color='green', alpha=0.3)\n",
        "            plt.xlabel('Month')\n",
        "            plt.ylabel('Rent')\n",
        "            plt.title('Home Rent Prediction')\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            st.pyplot(plt)\n",
        "\n"
      ],
      "metadata": {
        "id": "6e7pqVdPQEcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zPGhbaqQFE1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}